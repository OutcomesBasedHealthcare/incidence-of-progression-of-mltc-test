{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### **01 - Incidence of MLTC**\r\n",
        "#### **01A - Subsegment configuration**\r\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "**Imports**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# required imports\r\n",
        "from pyspark.sql import functions as F  # noqa: N812\r\n",
        "\r\n",
        "# requires blank line after last import\r\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "**Parameter cell**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": [
          "parameters"
        ]
      },
      "source": [
        "# parameter cell\r\n",
        "incidence_schema = \"\"  # \"mltc_incidence_outputs_v40_20230331\"\r\n",
        "segmentation_schema = \"\"  # \"obh_segmentation_v40_20230331\"\r\n",
        "specification_schema = \"\"  # \"specification_v40_20230331\"\r\n",
        "\r\n",
        "# optional, can be blank\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Set parameters in Spark configuration with 'param.' prefix (for use in SQL cells)\r\n",
        "spark.conf.set(\"param.incidence_schema\", incidence_schema)\r\n",
        "spark.conf.set(\"param.segmentation_schema\", segmentation_schema)\r\n",
        "spark.conf.set(\"param.specification_schema\", specification_schema)\r\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "---\r\n",
        "#### **01A - Subsegment configuration**\r\n",
        "\r\n",
        "Some subsegments are excluded from this analysis, so a purpose-specific configuration is applied to restrict to the selected subset of subsegments.\r\n",
        "\r\n",
        "The following section creates two tables, which are copies of `dim_subsegment_combinations` and `breakdown_subsegment_combinations`, restricted to a subset of conditions.\r\n",
        "\r\n",
        "This allows for recalculation of the `condition_count` field for the selected subset, which is then used in subsequent analyses to identify progression from one state of MLTC to the next.\r\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "**a - Create config view to select subset of subsegments**\r\n",
        "\r\n",
        "- Use `lookup_subsegments` table to obtain `subsegment_code`, `_id`, `_name` and `_description`\r\n",
        "- Apply binary flags for whether each subsegment is included\r\n",
        "- Restrict output to only subsegments flagged for inclusion\r\n",
        "\r\n",
        "**Instructions**: Update true/false flags here to update config for the analysis\r\n",
        "\r\n",
        "<blockquote style=\"color: #D8000C; background-color: #FFD2D2; padding: 10px; border-left: 6px solid #D8000C;\">\r\n",
        "  <strong>⚠️ Warning:</strong> DROP TABLE is currently commented out, as this table does not need to be recreated each time the incidence analysis is run.\r\n",
        "</blockquote>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        }
      },
      "source": [
        "%%sql\r\n",
        "\r\n",
        "--DROP TABLE IF EXISTS ${param.incidence_schema}.config_subsegments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "outputs": [],
      "metadata": {
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\r\n",
        "\r\n",
        "--CREATE    TABLE     ${param.incidence_schema}.config_subsegments USING PARQUET AS\r\n",
        "CREATE OR REPLACE TEMPORARY VIEW config_subsegments AS\r\n",
        "          SELECT    ls.subsegment_code,\r\n",
        "                    ls.subsegment_id,\r\n",
        "                    ls.subsegment_name,\r\n",
        "                    ls.subsegment_description,\r\n",
        "                    cs.is_included_in_config\r\n",
        "          FROM     \r\n",
        "          VALUES    ('alcohol_dependence', TRUE),\r\n",
        "                    ('asthma', TRUE),\r\n",
        "                    ('atrial_fibrillation', TRUE),\r\n",
        "                    ('bronchiectasis', TRUE),\r\n",
        "                    ('cancer', TRUE),\r\n",
        "                    ('cerebrovascular_disease', TRUE),\r\n",
        "                    ('chronic_kidney_disease', TRUE),\r\n",
        "                    ('chronic_liver_disease', TRUE),\r\n",
        "                    ('chronic_pain', FALSE), -- excluded due to RA overlap\r\n",
        "                    ('copd', TRUE),\r\n",
        "                    ('coronary_heart_disease', TRUE),\r\n",
        "                    ('cystic_fibrosis', FALSE), -- excluded as present from birth\r\n",
        "                    ('depression', TRUE),\r\n",
        "                    ('diabetes', TRUE),\r\n",
        "                    ('epilepsy', TRUE),\r\n",
        "                    ('heart_failure', TRUE),\r\n",
        "                    ('hypertension', TRUE),\r\n",
        "                    ('inflammatory_bowel_disease', TRUE),\r\n",
        "                    ('multiple_sclerosis', TRUE),\r\n",
        "                    ('osteoarthritis', TRUE),\r\n",
        "                    ('osteoporosis', TRUE),\r\n",
        "                    ('parkinsons_disease', TRUE),\r\n",
        "                    ('peripheral_vascular_disease', TRUE),\r\n",
        "                    ('pulmonary_heart_disease', TRUE),\r\n",
        "                    ('rheumatoid_arthritis', TRUE),\r\n",
        "                    ('sarcoidosis', TRUE),\r\n",
        "                    ('serious_mental_illness', TRUE),\r\n",
        "                    ('sickle_cell_disease', FALSE), -- excluded as present from birth\r\n",
        "                    ('learning_disability', FALSE), -- excluded as present from birth\r\n",
        "                    ('physical_disability', TRUE),\r\n",
        "                    ('dementia', TRUE),\r\n",
        "                    ('intermediate_frailty_risk_hfrs', FALSE), -- excluded due to resolves\r\n",
        "                    ('high_frailty_risk_hfrs', FALSE), -- excluded due to resolves\r\n",
        "                    ('end_stage_renal_failure', FALSE), -- excluded due to LTC overlap\r\n",
        "                    ('severe_interstitial_lung_disease', TRUE),\r\n",
        "                    ('liver_failure', FALSE), -- excluded due to LTC overlap\r\n",
        "                    ('neurological_organ_failure', FALSE), -- excluded due to LTC overlap\r\n",
        "                    ('severe_copd', FALSE), -- excluded due to LTC overlap\r\n",
        "                    ('severe_heart_failure', FALSE), -- excluded due to LTC overlap\r\n",
        "                    ('incurable_cancer', FALSE) -- excluded due to LTC overlap\r\n",
        "                    AS cs (subsegment, is_included_in_config)         \r\n",
        "          INNER     JOIN ${param.specification_schema}.lookup_subsegments ls \r\n",
        "                    -- case insensitive match ↓\r\n",
        "                    ON UPPER(cs.subsegment) = UPPER(ls.subsegment_name) \r\n",
        "                    WHERE     cs.is_included_in_config IS TRUE\r\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "**b - Create dataframes**\r\n",
        "\r\n",
        "This section creates dataframes from the existing versions of `dim_subsegment_combinations` and `breakdown_subsegment_combinations`.\r\n",
        "\r\n",
        "These will be used for the creation of adapted versions of `dim_subsegment_combinations` and `breakdown_subsegment_combinations` below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "df_config_subsegments = spark.table(\"${param.incidence_schema}.config_subsegments\")\r\n",
        "\r\n",
        "df_dim_subsegment_combinations = spark.table(\r\n",
        "    \"${param.segmentation_schema}.dim_subsegment_combinations\",\r\n",
        ")\r\n",
        "\r\n",
        "df_breakdown_subsegment_combinations = spark.table(\r\n",
        "    \"${param.segmentation_schema}.breakdown_subsegment_combinations\",\r\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "**c - Create adapted version of `dim_subsegment_combinations`**\r\n",
        "\r\n",
        "Mapping the config to `dim_subsegment_combinations` is slightly complicated due to the pivoted nature of the table (one column per subsegment).\r\n",
        "\r\n",
        "- The following logic effectively checks for where `dim_subsegment_combinations` subsegment column names match subsegments within the config\r\n",
        "- Subsegment columns for subsegments not in the config are ignored\r\n",
        "- Once the relevant subsegment columns have been selected they are populated with the corresponding subsegment code\r\n",
        "- These are then concatenated together to create an updated (`new_`) combination code\r\n",
        "- Subsegment names are also concatenated to create an updated (`new_`) combination name\r\n",
        "- Finally subsegment columns are converted back to binary flags\r\n",
        "\r\n",
        "\r\n",
        "<blockquote style=\"color: #D8000C; background-color: #FFD2D2; padding: 10px; border-left: 6px solid #D8000C;\">\r\n",
        "  <strong>⚠️ Warning:</strong> condition_count logic may need to be updated if config changes (for definitional overlaps)\r\n",
        "</blockquote>\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\r\n",
        "\r\n",
        "CREATE SCHEMA IF NOT EXISTS ${param.incidence_schema}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Step 1: get subsegment_code and subsegment_name pairs from config\r\n",
        "## Lowercase subsegment_name for matching to dim_subsegment_combination later\r\n",
        "subsegment_name_to_code = {\r\n",
        "    row[\"subsegment_name\"]: row[\"subsegment_code\"]\r\n",
        "    for row in df_config_subsegments.select(\r\n",
        "        F.lower(F.col(\"subsegment_name\")).alias(\"subsegment_name\"),\r\n",
        "        \"subsegment_code\",\r\n",
        "    ).collect()\r\n",
        "}\r\n",
        "\r\n",
        "# Step 2: Identify columns to select from dim_subsegment_combinations\r\n",
        "## Existing subsegment combination columns will be renamed with old_ prefix\r\n",
        "## Otherwise select subsegment columns where those subsegments appear in the config\r\n",
        "## Where each subsegment column is true, the value is replaced with the subsegment_code\r\n",
        "## This is to enable concatenation of codes for the creation of the new\r\n",
        "## subsegment_combination_code later\r\n",
        "## Other columns are ignored\r\n",
        "columns_to_select = []\r\n",
        "additional_columns_with_rename = [\r\n",
        "    F.col(\"subsegment_combination_id\").alias(\"old_subsegment_combination_id\"),\r\n",
        "    F.col(\"subsegment_combination_code\").alias(\"old_subsegment_combination_code\"),\r\n",
        "    F.col(\"subsegment_combination_name\").alias(\"old_subsegment_combination_name\"),\r\n",
        "]\r\n",
        "\r\n",
        "for col_name in df_dim_subsegment_combinations.columns:\r\n",
        "    if col_name.lower() in subsegment_name_to_code:\r\n",
        "        column = (\r\n",
        "            F.when(F.col(col_name), subsegment_name_to_code[col_name.lower()])\r\n",
        "            .otherwise(None)\r\n",
        "            .alias(col_name)\r\n",
        "        )\r\n",
        "        columns_to_select.append(column)\r\n",
        "    elif col_name in [\r\n",
        "        \"subsegment_combination_id\",\r\n",
        "        \"subsegment_combination_code\",\r\n",
        "        \"subsegment_combination_name\",\r\n",
        "    ]:\r\n",
        "        continue\r\n",
        "\r\n",
        "# Step 3: Combine final columns for selection\r\n",
        "final_columns_to_select = additional_columns_with_rename + columns_to_select\r\n",
        "\r\n",
        "# Step 4: Select final columns\r\n",
        "df_filtered = df_dim_subsegment_combinations.select(*final_columns_to_select)\r\n",
        "\r\n",
        "# Step 5: Concatenate all subsegment codes into a new combination code\r\n",
        "subsegment_columns = [\r\n",
        "    col_name\r\n",
        "    for col_name in df_filtered.columns\r\n",
        "    if col_name.lower() in subsegment_name_to_code\r\n",
        "]\r\n",
        "df_filtered = df_filtered.withColumn(\r\n",
        "    \"new_subsegment_combination_code\",\r\n",
        "    F.concat_ws(\"\", *subsegment_columns),\r\n",
        ")\r\n",
        "\r\n",
        "# Step 6: Map lowercased subsegment names back to original case-sensitive versions\r\n",
        "name_mapping = {\r\n",
        "    row[\"subsegment_name\"].lower(): row[\"subsegment_name\"]\r\n",
        "    for row in df_config_subsegments.collect()\r\n",
        "}\r\n",
        "\r\n",
        "# Step 7: Concatenate case-sensitive subsegment names into a new combination name\r\n",
        "concat_names_expr = F.concat_ws(\r\n",
        "    \" \",\r\n",
        "    *[\r\n",
        "        F.when(F.col(col_name).isNotNull(), F.lit(name_mapping[col_name.lower()]))\r\n",
        "        for col_name in subsegment_columns\r\n",
        "    ],\r\n",
        ")\r\n",
        "df_filtered = df_filtered.withColumn(\r\n",
        "    \"new_subsegment_combination_name\",\r\n",
        "    concat_names_expr,\r\n",
        ")\r\n",
        "\r\n",
        "# Step 8: Count non-NULL subsegments across each row (new condition count)\r\n",
        "## Note - does not deal with LTC/EOL overlaps as not required for current config\r\n",
        "count_non_null_expr = sum(\r\n",
        "    [\r\n",
        "        F.when(F.col(col_name).isNotNull(), 1).otherwise(0)\r\n",
        "        for col_name in subsegment_columns\r\n",
        "    ],\r\n",
        ")\r\n",
        "df_filtered = df_filtered.withColumn(\"new_condition_count\", count_non_null_expr)\r\n",
        "\r\n",
        "# Step 9: Revert subsegment columns back to binary flags\r\n",
        "for col_name in df_filtered.columns:\r\n",
        "    if col_name.lower() in subsegment_name_to_code:\r\n",
        "        df_filtered = df_filtered.withColumn(col_name, F.col(col_name).isNotNull())\r\n",
        "\r\n",
        "# Step 10: Write as table\r\n",
        "df_filtered.write.mode(\"overwrite\").saveAsTable(\r\n",
        "    f\"{incidence_schema}.dim_subsegment_combinations_config\",\r\n",
        ")\r\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "**d - Create adapted version of `breakdown_subsegment_combinations`**\r\n",
        "\r\n",
        "This step creates the `breakdown_subsegment_combinations` table, which contains one row per subsegment (i.e. unpivoted), for each subsegment in the combination (rather than one column), from the selected subset of subsegments.\r\n",
        "- This step unpivots the dataframe created in the previous step\r\n",
        "- By using this dataframe, rows in the output table are already restricted to subsegments that exist in the config\r\n",
        "- The new `subsegment_combination_code` and `subsegment_combination_name` from the step above are also applied"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Step 1: Create a copy of df_filtered with unpivoted 'subsegment_name_lower' column\r\n",
        "## Identify list of subsegment columns by their data type (all boolean flags)\r\n",
        "boolean_columns = [col for col, dtype in df_filtered.dtypes if dtype == \"boolean\"]\r\n",
        "\r\n",
        "df_long_format = None\r\n",
        "\r\n",
        "# For each subsegment\r\n",
        "for col_name in boolean_columns:\r\n",
        "    # Filter rows where the subsegment column is True and select necessary columns\r\n",
        "    # Add subsegment column name as value within new subsegment_name_lower column\r\n",
        "    df_temp = df_filtered.filter(F.col(col_name)).select(\r\n",
        "        \"old_subsegment_combination_id\",\r\n",
        "        \"old_subsegment_combination_code\",\r\n",
        "        \"old_subsegment_combination_name\",\r\n",
        "        \"new_subsegment_combination_code\",\r\n",
        "        \"new_subsegment_combination_name\",\r\n",
        "        \"new_condition_count\",\r\n",
        "        F.lit(col_name).alias(\"subsegment_name_lower\"),\r\n",
        "    )\r\n",
        "\r\n",
        "    # Union the temporary dataframe with the accumulating dataframe\r\n",
        "    df_long_format = (\r\n",
        "        df_temp if df_long_format is None else df_long_format.union(df_temp)\r\n",
        "    )\r\n",
        "\r\n",
        "# Step 2: Perform case-insensitive join back to config\r\n",
        "df_joined = df_long_format.join(\r\n",
        "    df_config_subsegments,\r\n",
        "    F.lower(df_long_format[\"subsegment_name_lower\"])\r\n",
        "    == F.lower(df_config_subsegments[\"subsegment_name\"]),\r\n",
        "    \"inner\",\r\n",
        ")\r\n",
        "\r\n",
        "# Step 3: Select specified columns\r\n",
        "df_breakdown_subsegment_combinations = df_joined.select(\r\n",
        "    \"old_subsegment_combination_id\",\r\n",
        "    \"old_subsegment_combination_code\",\r\n",
        "    \"old_subsegment_combination_name\",\r\n",
        "    \"new_subsegment_combination_code\",\r\n",
        "    \"new_subsegment_combination_name\",\r\n",
        "    \"new_condition_count\",\r\n",
        "    \"subsegment_code\",\r\n",
        "    \"subsegment_id\",\r\n",
        "    \"subsegment_name\",\r\n",
        "    \"subsegment_description\",\r\n",
        ")\r\n",
        "\r\n",
        "# Step 4: Write as table\r\n",
        "df_breakdown_subsegment_combinations.write.mode(\"overwrite\").saveAsTable(\r\n",
        "    f\"{incidence_schema}.breakdown_subsegment_combinations_config\",\r\n",
        ")\r\n",
        ""
      ]
    }
  ],
  "metadata": {
    "save_output": true,
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    }
  }
}